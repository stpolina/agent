{
    "title": "Global Solutions vs. Local Solutions for the AI Safety Problem",
    "authors": "Alexey Turchin , David Denkenberger and Brian Patrick Green",
    "affiliations": "1\nScience for Life Extension Foundation, Prospect Mira 124-15, Moscow 129164, Russia\n2\nAlliance to Feed the Earth in Disasters (ALLFED), University of Alaska Fairbanks, Fairbanks, AK 99775, USA\n3\nMarkkula Center for Applied Ethics, Santa Clara University, Santa Clara, CA 95053, USA\n*\nAuthor to whom correspondence should be addressed.",
    "year": "2019",
    "url": "https://www.mdpi.com/2504-2289/3/1/16",
    "article type": "Article",
    "abstract": "There are two types of artificial general intelligence (AGI) safety solutions: global and local. Most previously suggested solutions are local: they explain how to align or “box” a specific AI (Artificial Intelligence), but do not explain how to prevent the creation of dangerous AI in other places. Global solutions are those that ensure any AI on Earth is not dangerous. The number of suggested global solutions is much smaller than the number of proposed local solutions. Global solutions can be divided into four groups: 1. No AI: AGI technology is banned or its use is otherwise prevented; 2. One AI: the first superintelligent AI is used to prevent the creation of any others; 3. Net of AIs as AI police: a balance is created between many AIs, so they evolve as a net and can prevent any rogue AI from taking over the world; 4. Humans inside AI: humans are augmented or part of AI. We explore many ideas, both old and new, regarding global solutions for AI safety. They include changing the number of AI teams, different forms of “AI Nanny” (non-self-improving global control AI system able to prevent creation of dangerous AIs), selling AI safety solutions, and sending messages to future AI. Not every local solution scales to a global solution or does it ethically and safely. The choice of the best local solution should include understanding of the ways in which it will be scaled up. Human-AI teams or a superintelligent AI Service as suggested by Drexler may be examples of such ethically scalable local solutions, but the final choice depends on some unknown variables such as the speed of AI progress. View Full-Text",
    "keywords": "Keywords: AI safety; existential risk; AI alignment; superintelligence; AI arms race",
    "publication history": "Received: 16 December 2018 / Revised: 2 February 2019 / Accepted: 15 February 2019 / Published: 20 February 2019"
}